# 적대적 생성 신경망(Generative Adversarial Networks)
### What is GAN
- GAN이란? 학습 데이터들의 분포를 학습해, 같은 분포에서 새로운 데이터를 생성할 수 있도록 DL 모델을 학습시키는 프레임워크
- 2014년 Ian Goodfellow가 개발했으며, Generative Adversarial Nets논문에서 처음 소개
- GAN은 생성자와 구분자로 구별되는 두가지 모델을 가지고 있는것이 특징
- 생성자의 역할은 실제 이미지로 착각되도록 정교한 이미지를 만드는 것이고, 구분자의 역할은 이미지를 보고 생성자에 의해 만들어진 이미지인지 실제 이미지인지 알아내는 것
- 모델을 학습하는 동안, 생성자는 더 진짜같은 가짜 이미지를 만들어내며 구분자를 속이려 하고, 구분자는 더 정확히 가짜/진짜 이미지를 구별할 수 있도록 노력
- 생성자가 학습 데이터들에서 직접 가져온 것처럼 보일정도로 완벽한 이미지를 만들어내고, 구분자가 생성자에서 나온 이미지를 50%의 확률로 가짜 혹은 진짜로 판별할 때, 균형상태에 도달
- $x$는 이미지로 표현되는 데이터
- $D(x)$는 구분자 신경망이고, 실제 학습데이터에서 가져온 $x$를 통과시켜 상수(scalar) 확률값을 결과로 출력
- 이때, 이미지 데이터의 경우, $D(x)$에는 3x64x64크기의 CHW 데이터가 입력
- 직관적으로 볼때, $D(x)$는 $x$가 학습데이터에서 가져온 것일 때 출력이 크고, 생성자가 만들어낸 $x$일때 작을 것
- $D(x)$는 전통적인 이진 분류기(binary classification)으로도 생각될 수 있음
- $z$를 정규분포에서 뽑은 잠재공간 벡터(laten space vector) : 정규분포를 따르는 n개의 원소를 가진 vector
- $G(z)$는 $z$벡터를 원하는 데이터 차원으로 대응시키는 신경망
- 이때 $G$의 목적은 $p_{data}$에서 얻을 수 있는 학습 데이터들의 분포를 추정하며, 모사한$p_g$의 분포를 이용해 가짜 데이터들을 생성
- $D(G(z))$는 $G$가 출력한 결과물이 실제 이미지일 0~1사이의 상수의 확률값
- $D$가 이미지의 참/거짓을 정확히 판별할 확률인 $logD(x)$에서 생성한 이미지를 $D$가 가짜로 판별할 확률인 $(log(1-D(G(z))))$를 최소화 시키려는 점에서, $D$와 $G$는 최대최소(minmax)게임을 하는 것과 같음
- 손실함수 : $\min\limits_{G}\max\limits_{D}V(D,G) = \Epsilon_{x~p_{data}(x)}[logD(x)] + \Epsilon_{z~p_z(z)}[log(1-D(G(z)))]$
- 이론적으로는, 이 최대최소게임은 $p_g = p_{data}$이고, 구분자에 입력된 데이터가 1/2의 무작위 확률로 참/거직이 판별될때 해답에 이름

### What is DCGAN
- GAN에서 직접적으로 파생된 모델로, 생성자와 구분자에서 합성곱 신경망(convolution)과 전치 합성곱 신경망(convolution-transpose)을 사용했다는 점이 차이
- 구분자에서는 convolution계층, batch norm계층, 그리고 LeakyReLU 활성함수가 사용
- 생성자는 convolutional-transpose 계층, 배치 정규화(batch norm)계층, 그리고 ReLU 활성함수가 사용
- 이떄, 전치 합성곱 신경망은 잠재공간 벡터로 하여금 이미지와 같은 차원을 갖도록 변환시켜주는 역할 수행

### 설정값
- dataroot : 데이터셋 폴더 경로
- workers : DataLoader에서 데이터를 불러올 때 사용할 쓰레드의 개수
- batch_size : 학습에 사용할 배치 크기
- image_size : 학습에 사용되는 이미지의 크기
- nc : 입력 이미지의 색 채널개수
- nz : 잠재공간 벡터의 원소들 개수
- ngf : 생성자를 통과할때 만들어질 특징 데이터의 채널개수
- ndf : 구분자를 통과할때 만들어질 특징 데이터의 채널개수
- num_epochs : 학습시킬 에폭 수
- lr : 모델의 학습률
- beta1 : Adam 옵티마이저에서 사용할 beta1 하이퍼파라미터 값
- ngpu : 사용가능한 gpu의 번호

## Model Architecture
### Weights initialize
- DCGAN 논문에서는 평균이 0이고 분산이 0.02인 정규분포를 사용해, 구분자와 생성자 모두 무작위 초기화를 진행하는 것이 좋다고 안내
- weight_init 함수는 매개변수로 모델을 입력받아 모든 합성곱 계층, 전치 합성곱 계층, 배치 정규화 계층을 조건대로 다시 초기화 수행

### 생성자
- 생성자 $G$는 잠재 공간 벡터 $z$를 데이터 공간으로 변환시키도록 설계
- 실제 모델에서는 스트라이드 2를 가진 전치 합성곱 계층들을 이어서 구성하는데, 각 전치 합성곱 계층 하나당 2차원 배치 정규화 계층과 relu 활성함수를 한 쌍으로 묶어서 사용
- 생성자의 마지막 출력 계층에서는 데이터를 tanh 함수에 통과시키는데, 이는 출력 값을 [-1, 1]사이의 범위로 조정하기 위함
- 이떄, 배치 정규화 계층을 주목할 필요가 있는데, DCGAN 논문에 의하면 이 계층이 경사하강법(gradient descent)의 흐름에 중요한 영향을 미치는 것으로 알려짐

![nn](https://tutorials.pytorch.kr/_images/dcgan_generator.png)

### 구분자
- 구분자 $D$는 입력 이미지가 진짜 이미지인지 판별하는 전통적인 이진 분류 신경망
- 이때 $D$는 3x64x64 이미지를 입력받아 Conv2d, BatchNorm2d, 그리고 LeakyReLU 계층을 통과시켜 데이터를 가공시키고, 마지막 출력에서 Sigmoid 함수를 이용하여 0~1 사이의 확률값으로 조정
- 이 아키텍쳐는 필요한 경우 더 다양한 레이어를 쌓을 수 있지만, 배치 정규화와 LeakyReLU, 특히 보폭이 있는(strided) 합성곱 계층을 사용하는 것에는 이유가 존재
- DCGAN 논문에서는 보폭이 있는 합성곱 계층을 사용하는 것이 신경망 내에서 스스로의 풀링(Pooling)함수를 학습하기 때문에, 데이터를 처리하는 과정에서 직접적으로 풀링 계층(MaxPool or AvgPooling)을 사용하는 것보다 더 유리하며 배치 정규화와 leaky relu 함수는 학습과정에서 $G$와 $D$가 더 효과적인 경사도(gradient)를 얻을 수 있음

### 손실함수와 옵티마이저
- 손실함수로는 Binary Cross Entropy loss 사용
- $l(x,y) = L = {l_1, \ldots, l_N}^T, l_n = -[y_n \centerdot logx_n + (1 - y_n) \centerdot log(1 - x_n)]$
- 참 라벨은 1로 두고, 거짓 라벨은 0으로 설정
- 각 라벨의 ㄱ밧을 정한건 GAN 논문에서 사용된 값들로, GAN을 구성할때의 관례
- 옵티마이저는 $D$와 $G$ 각각 구성
- DCGAN 논문에 서술된 대로, 두 옵티마이저는 모두 Adam을 사용하고 학습률은 0.00002, Beta1 값은 0.5
- 학습이 진행되는 동안 생성자의 상태를 알아보기 위하여, 프로그램이 끝날 때까지 고정된 잠재 공간 벡터를 생성
- 이 벡터들 역시 가우시안 분포에서 추출
- 학습 과정을 반복하면서 $G$에 주기적으로 같은 잠재공간 벡터를 입력하면, 그 출력값을 기반으로 생성자의 상태를 확인

## Training
### Part 1 - 구분자의 학습
- 구분자의 목적은 주어진 입력값이 진짜인지 가짜인지 판별하는 것
- 구분자는 "변화도(gradient)를 상승(ascending)시키며 훈련"
- 즉, $log(D(x)) + log(1 - D(G(z))$를 최대화
- 미니 배치(mini-batch)를 분리하여 사용한 개념 사용
- 먼저, 진짜 데이터들로만 이루어진 배치를 만들어 $D$에 통과시켜 그 출력값으로 $(log(D(x)))$의 손실값을 계산하고, 역전파 과정에서의 변화도들을 계산
- 두번째 스텝에서는 오로지 가짜 데이터들로만 이루어진 배치를 만들어 $D$에 통과시키고, 그 출력값으로 $(log(1-D(G(z))))$의 손실값을 계산해 역전파 변화도를 계산
- 이때, 두가지 스텝에서 나오는 변화도들은 축적(accumulate)시켜야함
### Part 2 - 생성자의 학습
- 오리지널 GAN에서 생선자는 $log(1 - D(G(z)))$를 최소화시키는 방향으로 학습했지만 충분한 변화도를 제공하지 못하여서 이를 해결하기 위해 $log(D(G(z)))$를 최대화 하는 방식으로 바꾸어서 진행
- 코드에서 구현하기 위해서는 Part1에서 한대로 구분자를 이용해 생성자의 출력값을 판별해주고, 진짜 라벨값을 이용해 $G$의 손실값을 계산
- 구해진 손실값으로 변화도를 구하고, 최종적으로 옵티마이저를 이용해 $G$의 가중치들을 업데이트
- 생성자가 만들어낸 가짜 이미지에 진짜 라벨을 사용하는 것이 직관적으로 위배가 될테지만, 이렇게 라벨을 바꿈으로써 $log(x)$라는 BCELoss의 일부분을 사용할 수 있게 함
- 마무리로 $G$의 훈련 상태를 알아보기 위하여 몇가지 통계적인 수치들과 fixed_noise를 통과시킨 결과를 화면에 출력
- Loss_D : 진짜 데이터와 가짜 데이터들 모두에서 구해진 손실값 $(log(D(x)) + log(1-D(G(z))))$
- Loss_G : 생성자의 손실값 $(log(D(G(z))))$
- D(x) : 구분자가 데이터를 판별한 확률값. 처음에는 1에 가까운 값이다가 G가 학습할수록 0.5 값에 수렴
- D(G(z)) : 가짜데이터들에 대한 구분자의 출력값. 처음에는 0에 가까운 값이다가, G가 학습할수록 0.5에 수렴
